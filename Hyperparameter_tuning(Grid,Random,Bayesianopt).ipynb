{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE HYPER-PARAMETERS. TUNING DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** max_depth : int or None, optional (default=None)**\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Ignored if max_leaf_nodes is not None.\n",
    "    \n",
    "- **min_samples_split : int, optional (default=2)**\n",
    "    The minimum number of samples required to split an internal node.\n",
    "\n",
    "- There are more hyper-parameters: \n",
    "  - help(\"sklearn.tree.DecisionTreeRegressor\")\n",
    "  - help(\"sklearn.tree.DecisionTreeClassifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, data is loaded, inputs go to X, outputs to y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import tree\n",
    "from scipy.stats import sem\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINING HYPER-PARAMETER TUNING AND MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of model evaluation and hyper-parameter tuning can be understood as an external loop that trains a model and tests the model, and an internal loop, where the training process consists on looking for the best hyper-parameters, and then obtaining the model with those best hyper-parameters.\n",
    "\n",
    "First, we are going to use **Holdout** (train/test) for model evaluation (external loop), and **3-fold crossvalidation** for hyper-parameter tuning (internal loop). Hyper-parameters will be adjusted with **Gridsearch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random_state = 0 for reproducibility\n",
    "# Holdout for model evaluation. 33% of available data for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 49 candidates, totalling 147 fits\n",
      "23.621531765022485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 147 out of 147 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Search space\n",
    "param_grid = {'max_depth': list(range(2,16,2)),\n",
    "              'min_samples_split': list(range(2,16,2))}\n",
    "\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "clf = GridSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=3, \n",
    "                   n_jobs=1, verbose=1)\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# At this point, clf contains the model with the best hyper-parameters found by gridsearch\n",
    "# and trained on the complete X_train\n",
    "\n",
    "# Now, the performance of clf is computed on the test partition\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the best hyper-parameters and their score (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 12, 'min_samples_split': 10}, 21.63609319641027)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_, -clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more control is needed when splitting data for grid search, we can use **KFold** to generate the crossvalidation iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=0, shuffle=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# random_state=0 for reproducibility\n",
    "cv_grid = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "cv_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can do hyper-parameter tuning with 3-fold crossvalidation again. Results are different because shuffling of data within gridsearch might be different in the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 49 candidates, totalling 147 fits\n",
      "20.845882154306768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 147 out of 147 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Search space\n",
    "param_grid = {'max_depth': list(range(2,16,2)),\n",
    "              'min_samples_split': list(range(2,16,2))}\n",
    "\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "clf = GridSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=cv_grid, \n",
    "                   n_jobs=1, verbose=1)\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# At this point, clf contains the model with the best hyper-parameters found by gridsearch\n",
    "# and trained on the complete X_train\n",
    "\n",
    "# Now, the performance of clf is computed on the test partition\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMIZED SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use **Randomized Search** instead of gridsearch. Only 20 hyper-parameter value combinations will be tried (budget=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "22.35062888533451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Search space\n",
    "param_grid = {'max_depth': list(range(2,16,2)),\n",
    "              'min_samples_split': list(range(2,16,2))}\n",
    "\n",
    "budget = 20\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "clf = RandomizedSearchCV(tree.DecisionTreeRegressor(), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         cv=cv_grid, \n",
    "                         n_jobs=1, verbose=1,\n",
    "                         n_iter=budget\n",
    "                        )\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# At this point, clf contains the model with the best hyper-parameters found by gridsearch\n",
    "# and trained on the complete X_train\n",
    "\n",
    "# Now, the performance of clf is computed on the test partition\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Randomized Search**, we can define the search space with statistical distributions, rather than using particular values as we did before. Below you can see how to use a uniform distribution on integers between 2 and 16 by means of *randint*. For continuous hyper-parameters we could use continuous distributions such as *uniform* or *expon* (exponential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "22.250927406129268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from scipy.stats import uniform, expon\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Search space with integer uniform distributions\n",
    "param_grid = {'max_depth': sp_randint(2,16),\n",
    "              'min_samples_split': sp_randint(2,16)}\n",
    "\n",
    "budget = 20\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "clf = RandomizedSearchCV(tree.DecisionTreeRegressor(), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         cv=cv_grid, \n",
    "                         n_jobs=1, verbose=1,\n",
    "                         n_iter=budget\n",
    "                        )\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# At this point, clf contains the model with the best hyper-parameters found by gridsearch\n",
    "# and trained on the complete X_train\n",
    "\n",
    "# Now, the performance of clf is computed on the test partition\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to do **model evaluation with 5-fold crossvalidation** and **hyper-parameter tuning with 3-fold crossvalidation**? This is called nested crossvalidation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html). There is an external loop (for evaluating models) and an internal loop (for hyper-parameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# random_state=0 for reproducibility\n",
    "# Evaluation of model (outer loop)\n",
    "cv_evaluation = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "from scipy.stats import uniform, expon\n",
    "\n",
    "# Search space\n",
    "param_grid = {'max_depth': list(range(2,16,2)),\n",
    "              'min_samples_split': list(range(2,16,2))}\n",
    "\n",
    "budget = 20\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "# This is the internal 3-fold crossvalidation for hyper-parameter tuning\n",
    "clf = RandomizedSearchCV(tree.DecisionTreeRegressor(), \n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         # 3-fold for hyper-parameter tuning\n",
    "                         cv=3, \n",
    "                         n_jobs=1, verbose=1,\n",
    "                         n_iter=budget\n",
    "                        )\n",
    "\n",
    "# This is the external 5-fold crossvalidation for model evaluation\n",
    "# Notice that clf is the model resulting of hyper-parameter tuning\n",
    "scores = -cross_val_score(clf, \n",
    "                            X, y, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            cv = cv_evaluation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.95506453 14.04830397 15.73059325 47.94659277 11.27318105]\n",
      "23.990747111818475 +- 13.792773921974007\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "# The mean of the 5-fold crossvalidation is the final score of the model\n",
    "print(scores.mean(), \"+-\", scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBTAINING THE FINAL MODEL (FOR DEPLOYMENT, OR FOR SENDING TO A COMPETITION, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at the end, we need a final model, we can get it by fitting clf to all the available data. Let us remember that clf does hyper.parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clfFinal = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL BASED OPTIMIZATION (BAYESIAN OPTIMIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-optimize (skopt) will be used for this: https://scikit-optimize.github.io. **Holdout** for model evaluation and **3-fold crossvalidation** for hyper-parameter tuning (with **Model Based Optimization** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "22.386386715341622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge scikit-optimize \n",
    "from skopt import BayesSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from scipy.stats import uniform, expon\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Search space with integer uniform distributions\n",
    "param_grid = {'max_depth': (2,16),\n",
    "              'min_samples_split': (2,16)}\n",
    "\n",
    "budget = 20\n",
    "# random.seed = 0 for reproducibility\n",
    "np.random.seed(0)\n",
    "clf = BayesSearchCV(tree.DecisionTreeRegressor(), \n",
    "                    param_grid,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    cv=3,    \n",
    "                    n_jobs=-1, verbose=1,\n",
    "                    n_iter=budget\n",
    "                    )\n",
    "\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# At this point, clf contains the model with the best hyper-parameters found by bayessearch\n",
    "# and trained on the complete X_train\n",
    "\n",
    "# Now, the performance of clf is computed on the test partition\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
